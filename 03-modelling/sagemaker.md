# Amazon SageMaker

- SageMaker is intended to manage the entire machine learning workflow
- SageMaker allows us to: fetch, clean and prepare data -> train and evaluate model deploy models and evaluate results in production -> repeat
- SageMaker Notebooks:
    - They are jupiter notebook instances running on EC2 machines
    - They have access to S3
    - They have access to libraries such as Scikit-Learn, Spark, Tensorflow
    - They have access to a wide variate of built-in modles
    - They have the ability to spin up training instances and deploy trained models
- Data prep on SageMaker:
    - Data usually comes from S3, the format varies with algorithms, of is RecordIO/Protobuf
    - We can ingest data from Athena, EMR, Redshift, Amazon Keyspaces DB
    - Apache Spark integrates with SageMaker
    - Scikit-Learn, numpy, pandas all at our disposal within the notebook
- SageMaker processing:
    - Processing jobs:
        - Copy data from S3
        - Spin up a processing container
        - Output processed data to S3
- SageMaker training:
    - We crate a training job:
        - We provide an URL to an S3 bucket with the training data
        - We provide an URL to an S3 bucket for the output
        - We provide a path to an ECR container with the training code
        - SageMaker will spin up ML compute resources
    - Training options:
        - Builtin training algorithms
        - Spark MLLib
        - Custom Python Tensorflow/MXNet code
        - PyTorch, Scikit-Learn, RLEstimator (reinforcement learning)
        - XGBoost, Hugging Face, Chainer
        - Our own Docker image
        - Algorithm purchased from AWS marketplace
- Deploying trained models:
    - We can save trained data to S3
    - We can deploy in two ways:
        - Persistent endpoint for making individual predictions on demand
        - SageMaker Batch Transform to get predictions for an entire dataset
    - Other options:
        - Inference Pipelines: used for more complex processing
        - SageMaker Neo: deploy to edge devices
        - Elastic Inference for accelerating deep learning models
        - Automatic scaling: increase the # of endpoints if needed
        - Shadow Testing: evaluates new models against currently deployed models to catch errors

## Linear Learner

- Linear regression:
    - Fit a line to our training data
    - Predictions based on that line
- It can handle both regression (numeric) predictions and classification predictions:
    - For classifications a linear threshold function is used
    - Can do binary or multi-class classification
- Input format:
    - RecordIO-wrapped protobuf (Float32 data only!) - most performant
    - CSV - first column is assumed to be the label
    - File or Pipe mode both supported
- Processing:
    - Training data must be normalized (all features are weighted the same)
    - Linear Learner can do the normalization automatically
    - Input data should be shuffled
- Training:
    - Uses stochastic gradient descent (SGD)
    - We can chose from optimization algorithms: Adam, AdaGrad, SGD, etc.
    - Multiple models can be optimized in parallel
    - We can tune L1, L2 regularization
- Important hyperparameters:
    - Balance_multiclass_weights: gives each class equal importance in loss functions
    - Learning_rate, mini_batch_size
    - L1 regularization
    - Wd - Weight decay (L2 regularization)
- Instance types:
    - Training:
        - Single or multi-machine CPU or GPU
        - I does help to have more than one machine/it does not help to have more than one GPU per machine

## XGBoost

- eXtreme Gradient Boosting
    - Boosted groups of decision trees
    - New trees made to correct the errors of previous trees
    - Uses gradient descent to minimize loss as new trees are added
- Can be used for classifications and for regressions (regression trees)
- Input format:
    - Accepts CSV, libsvm, recordIO-protobuf, Parquet
    - Models are serialized/deserialized with Pickle
    - Can be used as a framework within notebooks
        - Sagemaker.xgboost
    - Or as a built-in SageMaker algorithm
- Important hyperparameters:
    - Subsample: prevent overfitting
    - Eta: step size shrinkage, used to prevent overfitting
    - Gamma: minimum loss reduction to create a partition; larger =  more conservative
    - Alpha: L1 regularization; larger =  more conservative
    - Lambda: L2 regularization; larger =  more conservative
    - eval_metric:
        - Optimize an AUC, error, rmse
        - For example, if we care about false positives more than accuracy, we might use AUC here
    - scale_pos_weight:
        - Adjust balance of positive and negative weights
        - Helpful for unbalanced classes
        - Might set to sum(negative cases)/sum(positive cases)
    - max_depth:
        - Max depth of a tree
        - Too high value can cause overfit
- Instance types:
    - Uses CPU's only for multiple instances, does not support GPU for that
    - Is memory-bound, not compute-bound (M5 is a good choice)
    - As of XGBoost 1.2, single-instance GPU training is available (P2, P3)
        - We must set tree_method hyperparameter to gpu_hist
        - It trains more quickly and can be more cost effective

## Seq2Seq

- Input is a sequence of tokens, output is a sequence of tokens
- Used for:
    - Machine translation
    - Text summarization
    - Speech to text
- It is implemented with RNNs and CNNs with attention
- Input format:
    - Expects RecordIO-Protobuf format: tokens must be integers (this is unusual since most algorithms expect floating point data)
    - We need to provide tokenized text files (we cannot provide a simple Word file, for example)
    - We can convert data to Protobuf format using sample code provided
    - We must provide: training data, validation data and vocabulary files
- How it is used?
    - Training for machine translation can take days
    - Pre-trained models are available
    - Public training datasets are available for specific translation tasks
- Important Hyperparameters:
    - Batch_size
    - Optimizer_type: adam, sgd, rmsprop
    - Learning_rate
    - Num_layers_encoder
    - Num_layers_decoder
    - We can optimize for:
        - Accuracy
        - BLUE score: compare our translation against multiple reference translations
        - Perplexity: cross-entropy metric
- Instance Types:
    - Can only use GPU instance types (P3 for example)
    - Can only use a single machine for training - it can use multi-GPUs on one machine

## DeepAR

- Used for forecasting one-dimensional time series data
- Uses RNNs
- Allows us to train the same model over several related time series
- Can find frequencies and seasonality
- Input format:
    - JSON lines format: can be GZIP of Parquet
    - Each record must contain:
        - Start: Starting timestamp
        - Target: time series values
    - Optionally, each record can contain:
        - Dynamic_feat: dynamic features such as was a promotion applied to a product ina time series, product purchases
        - Cat: categorical feature
- How is it used?
    - We always include entire time series for training, testing and inference
    - We always use the entire dataset as test set, remove last time points for training. We evaluate on withheld values
    - We don't want to use large values for prediction length (>400)
    - Train on many time series and not just one when possible
- Important hyperparameters:
    - Context_length
        - Number of time points the model sees before making a prediction
        - Can be smaller than seasonalities; the model will lag one year anyhow
    - Epochs
    - mini_batch_rate
    - Learning_rate
    - Num_cells
- Instance Types:
    - We can use CPU or GPU machines
    - We can have single or multi-machine clusters
    - Recommendation: start with CPU (ml.c4.2xlarge, ml.c4.4xlarge), move upt to GPU if necessary (large models or large mini-batch sizes >512)
    - For inference only CPU supported
    - May need larger instances for tuning

## BlazingText

- Can be used for coupe of different things:
    - Text classification:
        - Predict labels for a sentence, if we train the system with existing sentences and with the labels associated with them. It's a supervised learning
        - Useful in web searches, information retrieval (intended to be used for sentences, not for entire documents)
    - Word2vec
        - Creates a vector representation of words
        - Semantically similar words are represented by vectors close to each other (word embedding)
        - It is useful for NLP, but is not an NLP algorithm in itself
        - Can be used for machine translation, sentiment analysis
- Input format:
    - For supervised mode (text classification):
        - One sentence per line
        - First "word" in the sentence is the string `__label__` followed by the actual label for the sentence
    - Accepts "augmented manifest text format"
    - For Word2vec just wants a text file with one training sentence per line
- How is it used?
    - Word2vec has multiple modes:
        - Cbow (Continuous Bag of Words)
        - Skip-gram
        - Batch skip-gram
            - Distributed computation over many CPU nodes
- Important hyperparameters:
    - Word2vec:
        - Mode: batch_skipgram, skipgram, cbow
        - Learning_rate
        - Window_size
        - Vector_dim
        - Negative_samples
    - Text classification:
        - Epochs
        - Learning_rate
        - Word_ngrams
        - Vector_dim
- Instance Types:
    - For cbow and skipgram, recommended a single ml.p3.2xlarge (any single CPU or single GPU instance will work)
    - For batch_skipgram, we can use single or multiple CPU instances
    - For text classification, recommended is a c5 node for less than 2GB of training data. For larger datasets, it is recommended to use a single GPU instance (ml.p2.xlarge, ml.p3.2xlarge)

## Object2Vec

- It creates low-dimensional dense embeddings of high-dimensional objects
- It is basically Word2vec, generalized to handle things other than just words
- Can compute nearest neighbors of objects
- Can be used for genre prediction, recommendations, etc.
- Input format:
    - Data must be tokenized into integers
    - Training data consists of pairs of tokens and/or sequences of tokens, for example:
        - Sentence-sentence
        - Labels-sequence (genre description?)
        - Customer-customer
        - Product-product
        - User-item
- How it is used?
    - We train it with 2 input channels, two encoders and a comparator
    - Encoder choices:
        - Average-pooled embeddings
        - CNN's
        - Bidirectional LSTM
    - Comparator is followed by a feed-forward neural network
- Important hyperparameters:
    - Usual deep-learning parameters: dropout, early stopping, epochs, learning rate, batch size, layers, activation function, optimizer, weight decay
    - Enc1_network, enc2_network: we choose the encoder type: hcnn, bilstm, pooled_embedding
- Instance Types:
    - Can only train on a single machine: CPU or GPU instances are supported, multi-GPU is also supported
    - Recommended instance types:
        - CPU: ml.m5.2xlarge, ml.p2.xlarge
        - GPU: P2, P3, G4dn, G5
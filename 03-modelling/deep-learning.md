# Deep Learning

- Deep Learning frameworks:
    - Tensorflow / Keras
    - Apache MXNet

- Types of neural networks:
    - Feedforward Neural Network
    - Convolutional Neural Network (CNN):
        - Mainly used for 2D data, example image classification
    - Recurrent Neural Network (RNN):
        - Manly used for sequences in time or for things which have an order for them, example stock prediction, understand words of sentences, translation
        - Flavors of RNN: LSTM, GRU

## Activation Functions

- It is a function inside of a given neuron, that sums up the all the inputs and decides what output should be sent to the next layer of neurons
- Types of activation functions:
    - Linear: 
        - It doesn't really "do" anything, it outputs the input data
        - Can't do backpropagation
        - There is no point in having more than one layer of liner activation functions
    - Binary Step function:
        - It's an ON/OFF function
        - It can't handle multiple classification - it is a binary function
        - Vertical slopes don't work well with calculus (derivate is infinite)
    - Non-Linear activation functions:
        - They can create complex mappings between input and outputs
        - They allow backpropagation (they have useful derivatives)
        - They allow to have multiple layers
- Non-linear activation functions:
    - Sigmoid or Logistic / TanH (hyperbolic tangent):
        - They are nice and smooth
        - Sigmoid will scale the input between 0 to 1, TanH will scale the input between -1 to 1. They change slowly for high or low values ("Vanishing Gradient")
        - They are computationally expensive
        - Tanh is generally preferred over sigmoid
    - Rectified Linear Unit (ReLU):
        - Very popular choice for activation function
        - Very easy and fast to compute
        - In case the input are zero or negative, we have a linear function and all of its problems ("Dying ReLU problem")
    - Leaky ReLU:
        - Solves the "Dying ReLU problem" by introducing a negative slope bellow 0
    - Parametric ReLU (PReLU):
        - ReLU, but the slope is the negative part is learned via backpropagation
        - Complicated and computationally intensive
    - Other ReLU variants:
        - Exponential Linear Unit  (ELU)
        - Swish
            - From Google, performs really well
            - Performs well mostly with very deep networks (40+ layers)
        - Maxout
            - Output the max of the inputs
            - Technically ReLU is a special case of maxout
            - Doubles the parameters that need to be trained, not often practical
    - Softmax:
        - Used on the final output layer of a multiple classification problem
        - Basically converts outputs to probabilities of each classification
        - Can't produce more than one label for something (sigmoid can)
- Choosing an activation function:
    - For multiple classification, we should use softmax on the output layer
    - RNNs do well with Tanh
    - For everything else
        - Start with ReLU
        - If we need to do better, we should try Leaky ReLU
        - Last resort: PReLU, Maxout
        - Swish for really deep networks

## Convolutional Neural Networks (CNN)

- They are used mostly for image analysis
- Recommended when we have data that doesn't neatly align into columns. Examples:
    - Images that we want to find features within
    - Machine translation
    - Sentence classification
    - Sentiment analysis
- They can find features that aren't in a specific spot, examples:
    - Stop sing in a picture
    - Words within a sentences
- They are "feature-location invariant"
- How dod they work:
    - Local receptive fields are groups of neurons that only respond to a part of what is seen (subsampling)
    - They over overlap each other to cover the entire visual field  (convolution)
    - They feed into higher layers that identify increasing complex images
    - For color images we can used extra layers for red, green, and blue channels
- Building a CNN with Keras:
    - Source data must be of appropriate dimensions
    - Conv2D layer types does the actual convolution on a 2D image
    - MaxPooling2D layers can be used to reduce a 2D layer down by taking the maximum value in a given block
    - Flatten layers will convert the 2D layer to a 1D layer for passing into a flat hidden layer of neurons
    - Typical usage:
        - Conv2D -> MaxPooling -> Dropout -> Flatten -> Dense -> Dropout -> Softmax
- CNNs are very computationally intensive (CPU, GPU, adn RAM)
- CNNs have a lot of hyperparameters to configure (kernel sizes, layers with different number of units, amount of pooling, etc.)
- They are specialized architectures of CNNs:
    - LeNet-5: handwriting recognition
    - AlexNet: image classification
    - GoogLeNet: deeper than AlexNet, introduces inception modules (groups of convolutional layers)
    - ResNet (Residual Network): even deeper - maintains performance via skip connections